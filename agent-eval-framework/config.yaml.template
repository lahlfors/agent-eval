# The import path to the adapter class that will be used to interact with the agent.
agent_adapter_class: "eval.adapters.VertexAgentEngineAdapter"

# Configuration specific to the chosen agent adapter.
agent_config:
  # The AGENT_ENGINE_ID for the deployed Vertex AI Agent Engine.
  # Note: The script will prioritize getting this value from the AGENT_ENGINE_ID environment variable.
  agent_engine_id: "placeholder-id"

# The local or GCS path to the golden dataset.
dataset_path: "eval/vertex_eval_data/golden_record.jsonl"

# The list of metrics to run using the Vertex AI Evaluation Service.
# This is a list of dictionaries, where each dictionary configures a single metric.
metrics:
  # Example 1: Simple computation-based metric (like the old format)
  - name: "exact_match"
    type: "computation"

  # Example 2: Built-in, managed rubric-based metric
  - name: "text_quality"
    type: "rubric"
    # Optional: specify a version for the rubric if needed
    version: "v1"

  # Example 3: Rubric-based metric with custom guidelines
  - name: "custom_quality_check"
    type: "rubric"
    # Use a predefined spec and provide custom parameters
    predefined_spec_name: "general_quality"
    metric_spec_parameters:
      guidelines: "The response must be professional and not give financial advice."

  # Example 4: Custom function metric
  - name: "keyword_check"
    type: "custom_function"
    # The full import path to the custom evaluation function
    custom_function_path: "my_project.my_metrics.contains_keyword"

# (Optional) A mapping from the framework's internal column names to the
# column names in your dataset file. This allows you to use datasets that
# don't follow the default naming convention.
# column_mapping:
#   prompt: "your_prompt_column_name"
#   reference_response: "your_reference_column_name"
#   reference_trajectory: "your_trajectory_column_name"
